{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "config = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], config)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "#        for gpu in gpus:\n",
    "#          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "#        print(e)\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import csv\n",
    "print(cv.__version__)\n",
    "import glob\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "print('Step 1: reading images and labels')\n",
    "samples = []\n",
    "\n",
    "images = []\n",
    "data_set_no = 1\n",
    "data_set_len = np.zeros(data_set_no, dtype = np.int16)\n",
    "\n",
    "for i in range(data_set_no):\n",
    "    label_file_name = '/home/hsaeidi/nn_data_for_breathing_tracker/set' + str((i+1))+'/labels/lebels.csv' \n",
    "    with open(label_file_name) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            samples.append(line)\n",
    "    if i == 0:\n",
    "        data_set_len[i] = len(samples)\n",
    "    else:\n",
    "        data_set_len[i] = len(samples) - data_set_len[i-1]\n",
    "    address_name = '/home/hsaeidi/nn_data_for_breathing_tracker/set' + str((i+1))+'/images/img'\n",
    "    for j in range(data_set_len[i]):\n",
    "        tmp = address_name + str((j+1)) +'.png'\n",
    "        images.append(tmp)\n",
    "#read images # read the image addresses\n",
    "print(data_set_len)\n",
    "sample_size = len(samples)\t\n",
    "print('Total number of collected samples: ',sample_size)\n",
    "images_no = len(images)\n",
    "print('Total number of image addresses: ', images_no)\n",
    "tt = np.cumsum(data_set_len)\n",
    "print(tt)\n",
    "ttt = np.insert(tt, 0, 0)\n",
    "print(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "\n",
    "# read some sample images for a quick test\n",
    "img1 = Image.open(str(images[0]))\n",
    "img2 = Image.open(str(images[63]))\n",
    "\n",
    "print('The sample image size is')\n",
    "print(img1.size)\n",
    "print(' and it contains this type of data')\n",
    "print(np.asarray(img1))\n",
    "print('when normalized it gets like this')\n",
    "print(np.asarray(img1)/float(255))\n",
    "\n",
    "\n",
    "#plot 2 of them\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img1)\n",
    "plt.title('img 1')\n",
    "plt.subplot(132)\n",
    "plt.imshow(img2)\n",
    "plt.title('img 2')\n",
    "\n",
    "\n",
    "# see how the summation looks like\n",
    "plt.subplot(133)\n",
    "img_last = Image.open(str(images[86]))\n",
    "plt.imshow(img_last)\n",
    "plt.title('half a cycle away frame')\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "# preparing some quick training data\n",
    "x_raw = []\n",
    "y_raw = []\n",
    "moving_no = 0\n",
    "stopped_no = 0\n",
    "# calculate: f(n) - sum_(i = n-13)^(i = n-1) f(i), summations and f(n) - f(n-13)\n",
    "data_offset = np.cumsum(data_set_len)\n",
    "print(data_offset)\n",
    "data_offset = np.insert(data_offset, 0, 0)\n",
    "print(data_offset)\n",
    " \n",
    "t_h = 14\n",
    "t_h_1 = t_h -1\n",
    "\n",
    "for k in range(data_set_no):\n",
    "    for i in range(data_offset[k] + t_h_1 , data_offset[k+1]):\n",
    "        x_d_all = np.zeros(img1.size) + np.asarray(Image.open(str(images[i])))/float(255)\n",
    "        x_sum_all = x_d_all \n",
    "        x_last2first = x_d_all - np.asarray(Image.open(str(images[i-t_h_1])) )/float(255)\n",
    "        for j in range(1,t_h):\n",
    "            img_tmp = Image.open(str(images[i-j]))\n",
    "            x_d_all -= np.asarray(img_tmp)/float(255)\n",
    "            x_sum_all += np.asarray(img_tmp)/float(255)\n",
    "        img_combined = cv.merge((x_d_all, x_last2first, x_sum_all))\n",
    "        #x_raw.append(img_combined)\n",
    "        # put the corresponding label: 1 when stopped breathing and 0 when breathing\n",
    "        if samples[i] == ['1']:\n",
    "            y_label = [0.0, 1.0]\n",
    "            x_raw.append(img_combined)\n",
    "            y_raw.append(y_label)\n",
    "            stopped_no += 1    \n",
    "            #print('a stopped breathing sample')\n",
    "        else:\n",
    "            y_label = [1.0, 0.0]  \n",
    "            if (i%3) == 0:\n",
    "                x_raw.append(img_combined)\n",
    "                y_raw.append(y_label)\n",
    "                moving_no += 1\n",
    "# some info about the new processed data for training\n",
    "#print(y_raw)\n",
    "print('size of train data after calculations ',len(x_raw))\n",
    "#print(' which is the total of ', sample_size, ' minus ', data_set_no,'x', '= ', data_set_no*13 )\n",
    "print(' and totally ', moving_no, ' breathing samples vs', stopped_no, ' stopped breathing samples')\n",
    "print('---------')\n",
    "stopped_index = 0\n",
    "print(len(y_raw))\n",
    "y_tmp = np.array(y_raw)\n",
    "for i in range(0,len(y_tmp)):\n",
    "    #print(i)\n",
    "    #print(y_tmp[i])\n",
    "    if y_tmp[i][1] == '1.0':\n",
    "        stopped_index = i\n",
    "        break\n",
    "print(stopped_index)\n",
    "print('---------')\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_raw[stopped_index+t_h][:,:,0])\n",
    "plt.title('moving (x_d_all)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_raw[stopped_index+t_h][:,:,1])\n",
    "plt.title('moving (last - first)')\n",
    "plt.subplot(133)\n",
    "plt.imshow(x_raw[stopped_index+t_h][:,:,2])\n",
    "plt.title('moving (sum all)')\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_raw[stopped_index][:,:,0])\n",
    "plt.title('stopped (x_d_all)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_raw[stopped_index][:,:,1])\n",
    "plt.title('stopped (last - first)')\n",
    "plt.subplot(133)\n",
    "plt.imshow(x_raw[stopped_index][:,:,2])\n",
    "plt.title('stopped (sum all)')\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_raw, y_raw, test_size=0.2, random_state=1, shuffle= True)\n",
    "\n",
    "print('train data size and type:')\n",
    "print(len(x_train))\n",
    "print(type(x_train[0]))\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "#normalized the inputs between 0-1\n",
    "x_train_normalized = []\n",
    "x_val_normalized = []\n",
    "\n",
    "scale = float(1)/1\n",
    "print(scale)\n",
    "for i in range(0,len(x_train)):\n",
    "    x_train_normalized.append(x_train[i]*scale)\n",
    "for i in range(0,len(x_val)):\n",
    "    x_val_normalized.append(x_val[i]*scale)\n",
    "\n",
    "# test the normalized data\n",
    "print('an example of the normalized inputs')\n",
    "print(x_train_normalized[0])\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('before normalization')\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_train_normalized[0])\n",
    "plt.title('normalized')\n",
    "\n",
    "\n",
    "x_train_normalized = np.array(x_train_normalized)\n",
    "print('shape of normalized train inputs')\n",
    "print(x_train_normalized.shape)\n",
    "x_train_normalized = x_train_normalized.reshape(x_train_normalized.shape[0], 128, 128, 3)\n",
    "print('reshaped normalized train input dimensions')\n",
    "print(x_train_normalized.shape)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val_normalized = np.array(x_val_normalized)\n",
    "print('shape of normalized validation inputs')\n",
    "print(x_val_normalized.shape)\n",
    "x_val_normalized = x_val_normalized.reshape(x_val_normalized.shape[0], 128, 128, 3)\n",
    "print('reshaped normalized validation input dimensions')\n",
    "print(x_val_normalized.shape)\n",
    "y_val = np.array(y_val)\n",
    "#print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing other useful packages for training\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#from keras.optimizers import SGD\n",
    "# are the following still working in tf 2.1?\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# From the filtered and resized image (160x80x2), crop some less useful pixels from top an bottom for faster processing\n",
    "crop_top = 0\n",
    "crop_bottom = 0\n",
    "# the format of final image that goes to the first convolution layer \n",
    "ch, row, col = 3, 128-crop_top-crop_bottom, 128  # Trimmed image format\n",
    "\n",
    "# Neural network architecture\n",
    "model = Sequential()\n",
    "# convolution layers that gradually become deeper\n",
    "model.add(Conv2D(24, kernel_size = (5, 5), strides =(2,2), padding='same', activation='relu', input_shape=(row, col, ch)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(36, kernel_size = (5, 5), strides =(2,2), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(48, kernel_size = (5, 5), strides =(2,2), padding='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), strides =(2,2), padding='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "#removed temp:model.add(Conv2D(64, kernel_size = (3, 3), strides =(2,2), padding='same', activation='relu'))\n",
    "#removed temp:model.add(Dropout(0.25))\n",
    "# flattening the outputs and using dense layers up to the final output (i.e. steering angle)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# outputting the moving/stopped\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print('model defined')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer='adam',  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              #loss='binary_crossentropy',\n",
    "              loss='categorical_crossentropy',\n",
    "              # List of metrics to monitor\n",
    "              metrics=[\"accuracy\"])\n",
    "print('compiled the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training started')\n",
    "history = model.fit(x_train_normalized, y_train,\n",
    "\t\t                batch_size=512,\n",
    "\t\t                epochs=100,\n",
    "                        shuffle=True,\n",
    "\t\t                # We pass some validation for\n",
    "\t\t                # monitoring validation loss and metrics\n",
    "\t\t                # at the end of each epoch\n",
    "\t\t                validation_data=(x_val_normalized, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../saved_models/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ind = 37\n",
    "yy = model.predict(x_train_normalized[test_ind].reshape(1, 128, 128, 3), batch_size=1)\n",
    "print(yy)\n",
    "print(y_train[test_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some psuedo image inputs for quick test\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 1))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 1))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(type(x_train))\n",
    "print(x_train[0].shape)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "#model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), strides =(2,2), padding='same', activation='relu', input_shape=(100, 100, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = bce([0., 0., 1., 1.], [0., 0., 1., 0.])\n",
    "print('Loss: ', loss.numpy()) \n",
    "\n",
    "test_a = np.zeros((3, 2, 2))\n",
    "print(test_a)\n",
    "print(test_a.shape)\n",
    "\n",
    "img_combined = cv.merge((test_a[0], test_a[1], test_a[2]))\n",
    "\n",
    "print(img_combined.shape)\n",
    "for j in range(1,14):\n",
    "    print(j)\n",
    "print(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
